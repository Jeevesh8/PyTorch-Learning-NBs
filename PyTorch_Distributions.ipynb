{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-Distributions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQHeI0GrQP6jO5+dd/kBnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/PyTorch-Learning-NBs/blob/master/PyTorch_Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzDLPFjzRXJV",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTANT LINKS** :-\n",
        "\n",
        "    https://bochang.me/blog/posts/pytorch-distributions/\n",
        "\n",
        "    https://pytorch.org/docs/stable/distributions.html#score-function\n",
        "\n",
        "    https://pytorch.org/docs/stable/distributions.html#pathwise-derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1HmaWGNycNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "f731911c-66da-4c58-b8fc-788577ea6113"
      },
      "source": [
        "import torch\n",
        "\n",
        "mu = torch.tensor([[1.,2, 3],[4,5,6]])\n",
        "sigma = torch.tensor([[2,3,4],[1,2.,1]])\n",
        "dis = torch.distributions.normal.Normal(mu, sigma)\n",
        "samples = dis.rsample((2,)) #[2 samples] Reparametrized Sample(Gradient passes back through mu and sigma if mu and sigma can pass gradient any backward)\n",
        "print(samples)    \n",
        "print(dis.sample((2,)))     #Simple sample"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.9505,  3.2115, -1.4568],\n",
            "         [ 4.3786,  6.8014,  5.4601]],\n",
            "\n",
            "        [[ 3.4084,  7.0948, 11.5182],\n",
            "         [ 2.4940,  8.0826,  5.5930]]])\n",
            "tensor([[[-0.3580, -0.1331, -3.6138],\n",
            "         [ 4.9757,  2.4498,  7.5045]],\n",
            "\n",
            "        [[ 0.1587,  5.2897,  4.4107],\n",
            "         [ 4.0395,  1.8586,  7.0182]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T1-BnVy1e1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e7b7cc4d-690a-4903-a554-079d569c586c"
      },
      "source": [
        "x = torch.tensor([0,1.,2.])\n",
        "print(x,x.exp())\n",
        "print(dis.log_prob(samples), dis.log_prob(samples).exp())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2.]) tensor([1.0000, 2.7183, 7.3891])\n",
            "tensor([[[-1.7250, -2.0991, -2.9260],\n",
            "         [-0.9906, -2.0177, -1.0647]],\n",
            "\n",
            "        [[-2.3372, -3.4596, -4.5727],\n",
            "         [-2.0530, -2.7999, -1.0018]]]) tensor([[[0.1782, 0.1226, 0.0536],\n",
            "         [0.3714, 0.1330, 0.3448]],\n",
            "\n",
            "        [[0.0966, 0.0314, 0.0103],\n",
            "         [0.1283, 0.0608, 0.3672]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPsiPOhKOMi",
        "colab_type": "code",
        "outputId": "ee3eec5e-1cae-4992-fe50-14fe9f0d04a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "relu = torch.nn.ReLU()\n",
        "x = torch.tensor([1.,2,3], requires_grad=True)\n",
        "y = torch.tensor([4.,5,6], requires_grad=True)\n",
        "optimizer = torch.optim.Adam([x,y], lr=0.01)\n",
        "\n",
        "    \n",
        "iters = 1000\n",
        "\n",
        "for i in range(iters) :\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    m = relu(x)                                                                 #Don't write x=relu(x) as, old x would be deleted then and x.grad==0 after optimizer.step()\n",
        "    n = relu(y)\n",
        "    \n",
        "    tar_dis = torch.distributions.multinomial.Multinomial(1, m)\n",
        "    dis = torch.distributions.multinomial.Multinomial(1, n)\n",
        "    \n",
        "    if i==0 or i==iters-1 :\n",
        "        print(dis.probs, tar_dis.probs)\n",
        "    \n",
        "    a = dis.log_prob(dis.sample())\n",
        "    b = tar_dis.log_prob(tar_dis.sample())\n",
        "    \n",
        "    #print(a,b)\n",
        "    loss = (a-b)*(a-b)\n",
        "    \n",
        "    #print(loss)\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    #print(x.grad, y.grad)\n",
        "    \n",
        "    x.detach_()\n",
        "    y.detach_()\n",
        "    x.requires_grad = True                                                      #After detach, requires_grad must be set true\n",
        "    y.requires_grad = True\n",
        "    \n",
        "    #print(x, dis.probs)\n",
        "    #print(y, tar_dis.probs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2667, 0.3333, 0.4000], grad_fn=<DivBackward0>) tensor([0.1667, 0.3333, 0.5000], grad_fn=<DivBackward0>)\n",
            "tensor([0.3333, 0.3333, 0.3333], grad_fn=<DivBackward0>) tensor([0.3333, 0.3333, 0.3333], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC9UOqpxLPdL",
        "colab_type": "code",
        "outputId": "6a173a67-1fe3-4c42-da2e-42ddd23a063e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from torch.distributions.kl import kl_divergence as kld \n",
        "\n",
        "mean_1 = torch.tensor([1., 2, 3], requires_grad=True)\n",
        "mean_2 = torch.tensor([2., 2, 3], requires_grad=True)\n",
        "var_1 = torch.tensor([[1., 0, 0], [0., 1, 0], [0, 0, 0.7]], requires_grad=True)\n",
        "var_2 = torch.tensor([[1., 0, 0], [0., 1, 0], [0, 0, 0.8]], requires_grad=True)\n",
        "params = [mean_1, mean_2, var_1, var_2]\n",
        "optimizer = torch.optim.Adam(params, lr=0.01)\n",
        "\n",
        "iters=100\n",
        "for i in range(iters) :\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    \n",
        "    dis_1 = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, var_1)\n",
        "    dis_2 = torch.distributions.multivariate_normal.MultivariateNormal(mean_2, var_2)\n",
        "\n",
        "    loss = kld(dis_1, dis_2)\n",
        "    \n",
        "    if i==0 or i==iters-1 :\n",
        "        print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    for param in params :\n",
        "        param.detach_()\n",
        "        param.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5043, grad_fn=<AddBackward0>)\n",
            "tensor(8.4341e-06, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-KkToJdI5Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "60cb5e9e-cf68-4264-b92a-f32e0505bbbd"
      },
      "source": [
        "x = torch.randn((1,2,3))\n",
        "print(x)\n",
        "print(x.repeat_interleave(4, -2).reshape(1,2,4,3))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.9830,  1.3225,  0.9491],\n",
            "         [-0.6345, -1.1244,  1.5391]]])\n",
            "tensor([[[[-0.9830,  1.3225,  0.9491],\n",
            "          [-0.9830,  1.3225,  0.9491],\n",
            "          [-0.9830,  1.3225,  0.9491],\n",
            "          [-0.9830,  1.3225,  0.9491]],\n",
            "\n",
            "         [[-0.6345, -1.1244,  1.5391],\n",
            "          [-0.6345, -1.1244,  1.5391],\n",
            "          [-0.6345, -1.1244,  1.5391],\n",
            "          [-0.6345, -1.1244,  1.5391]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY5ypQdW-ZDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}